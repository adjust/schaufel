.TH SCHAUFEL.CONF 5
.SH NAME
schaufel.conf \(em configuration file for schaufel
.SH SYNOPSIS
.B /etc/schaufel/schaufel.conf
.SH DESCRIPTION
schaufel.conf uses libconfig syntax, which is well structured data which
also allows for comments. Definition of syntax is to be found within
libconfigs own documentation.
.PP
Every section may be overwritten by the command line and can therefore be
missing from your config file.
.SH CONFIG SECTIONS
Schaufel expects three sections: \fBlogger\fR, \fBconsumers\fR and
\fBproducers\fR.
.SS logger
The logger section defines the workings of schaufels logging.
.PP
It expects you to set a \fItype\fR. Types supported are
\fBsyslog\fR|\fBstderr\fR|\fBstdout\fR|\fBnull\fR|\fBfile\fR.
.PP
\fBfile\fR as a type supports \fBmode\fR as octal and requires \fBfile\fR
as a path. The standard mode is 0640.

.RS
.PP
 logger =
 {
    file = "/var/log/schaufel/schaufel.log";
    type = "file";
    mode = 0644;
 };
.RE
.PP
\fBsyslog\fR as a type supports \fBident\fR and \fBfacility\fR and strings.
\fBident\fR is the string your schaufel instance identifies as.
\fBfacility\fR is the logging facility, the default is facility \fIdaemon\fR.
.RS
.PP
 logger =
 {
    type = "syslog";
    ident = "schaufel_test";
    facility = "user";
 };
.RE
.PP

.SS consumers
Consumers are of the libconfig list type as there may be multiple
consumers defined. The necessary minimum configuration is a \fItype\fR
and the amount of \fIthreads\fR to be spawned.
.RS
.PP
consumers = (
    {
        type = "dummy";
        threads = 1;
    }
);
.RE
.PP
Most consumer types take extra configuration. What follows is an improbable
example of all types combined.
.RS
.PP
consumers = (
    {
        type = "kafka";
        threads = 10;
        broker = "kafka-1.host.name";
        groupid = "schaufel";
        topic = "schaufel_queue";
    },
    {
        type = "file";
        threads = 1;
        file = "/tmp/data.json"
    },
    {
        type = "redis";
        threads = 5;
        # optional pipeline size
        pipeline = 5000;
        host = "localhost:6379";
        topic = "data";
    }
);
.RE
.PP
.SS producers
Producers are, like consumers, a list of items which require a \fItype\fR
and \fIthreads\fR defined.
.RS
.PP
producers = (
    {
        type = "dummy";
        threads = 1;
    }
);
.RE
.PP
Most producers take extra configuration. Here's an example list of the inane
kind. Usually, you wouldn't want to produce to different data sinks.
Having a list is handy if you want to produce to a cluster.
.RS
.PP
producers = (
    {
        type = "kafka";
        threads = 10;
        broker = "kafka-1.host.name";
        topic = "schaufel_queue";
    },
    {
        type = "file";
        threads = 1;
        file = "/tmp/data.json"
    },
    {
        type = "redis";
        threads = 5;
        # optional pipeline size
        pipeline = 5000;
        host = "localhost:6379";
        topic = "data";
    },
    {
        type = "postgres";
        threads = 1;
        # topic and host name are mangled
        # into the table to be inserted to.
        # In this case that would be:
        # bagger_1_5432_test.data
        topic = "test";
        host = "bagger-1:5432";
    } );
.RE
.PP
Next to the standard producers, there are two special kinds. The postgres
with replication (\fBbagger\fR) kind, and the \fBexports\fR type.
.SS kafka
Kafka is a producer and consumer to Apache \fIkafka\fR, using \fIlibrdkafka\fR.
Only the message payload is forwarded, all metadata is discarded.
.PP
The miminum configuration for a producer is a broker and a topic, for the
consumer a broker, topic and group id are required.
Further options can be set if the groups \fIkafka_options\fR and
\fItopic_options\fR are added to the config file. The names of options
correspond to librdkafka and that is where they are documented.
.PP
Because of limitations in libconfigs grammar, dots are replaced by underscores
in the naming scheme.
.RS
.PP
consumers = (
    {
        type = "kafka";
        threads = 10;
        broker = "kafka-1.host.name";
        topic = "schaufel_queue";
        group = "schaufel_queue_1";
        kafka_options = {
            partition_assignment_strategy = "roundrobin";
            enable_auto_commit = "false;
        };
        topic_options = {
            auto_offset_reset = "beginning";
        };
    });
.PP
producers = (
    {
        type = "kafka";
        threads = 10;
        broker = "kafka-1.host.name";
        topic = "schaufel_new_queue";
        topic_options = {
            compression_codec = "zstd";
        };
    });

.RE
.SS postgres
The postgres producer can copy data into a table. The default format
is \fIbagger\fR (copying to a predefined schema). Alternatively, the format
can be given as \fBcsv\fR or \fBbinary\fR.
.PP
The producer commits every 2000 messages. No error handling is done after commit,
if commiting data should fail the data is lost.
.RS
producers = (
  {
    threads = 1;
    type = "postgres";
    host = "localhost:5432";
    dbname = "data";
    topic = "import";
    format = "csv";
  } );
.RE
.PP
Should the format be \fBbinary\fR, schaufel adds a binary header on its own
(as it commits every 2000 messages it needs to do so anyway). Please omit
any binary header.
.PP
.SS bagger
Bagger is essentially a producer of the \fIpostgres\fR type. Through the
\fIhost\fR string one can define a list of postgres databases and message
replication.
.PP
The table of the postgres producer is called \fIdata\fR and only has single
column. This needs to be able to store the messages, represented as text
(\fItext\fR and \fIjsonb\fR will do).
.RS
.PP
producers = (
    {
        type = "postgres";
        threads = 5;
        topic = "15";
        host = "bagger-1:5432,bagger-1:5433,bagger-1:5434;bagger-2:5432,bagger-2:5433,bagger-2:5434";
    } );
.RE
.PP
This configuration creates 5 threads per host specified. Hosts are separated
by commas. Hosts before the semicolon are masters, whereas the others
receive replicas of messages. Messages are distributed in no particular
order.

.SS exports
Exports is also a producer to postgres. Unlike bagger, it takes json data
and dereferences it into columns of a type. At the moment only
\fItext\fR and \fItimestamp\fR are supported. Feel free to add more types.
.PP
Dereferencing is done via a list of json pointers called \fIjpointers\fR.
These pointers confirm to \fIRFC 6901\fR. If a type other than text is
required, an array can be used to specify a type.
If a json pointer does not return data, the field is transformed to a
postgres null.
.RS
.PP
producers = (
    {
        type = "exports"
        threads = 1;
        topic = "data";
        jpointers = (
            "/data/customer",
            "/data/request/http_response",
            "/data/request/body/0",
            ["/timestamp", "timestamp"],
        );
    } );
.RE
.PP
If further filtering of the data should be required, this array can be extended
with actions and filters. The standard action is to \fIstore\fR, the standard
filter is \fInoop\fR.
.PP
A filter returns boolean true or false. An action takes this return value and
decides what to do with it. If the action is \fIstore\fR, it'll return true
no matter what the filter says. Filter \fInoop\fR will also always be true.
.PP
The datastructure above is transformed to look like this under the hood:
.PP
producers = (
    {
        type = "exports"
        threads = 1;
        topic = "data";
        jpointers = (
            [ "/data/customer", "text", "store", "noop" ],
            [ "/data/request/http_response", "text", "store", "noop" ],
            [ "/data/request/body/0", "text", "store", "noop" ],
            [ "/timestamp", "timestamp"],
        );
    } );
.RE
.PP
Exports supports a variety of actions and filters:
.PP
.TS
box, center, tab (@);
 c | c
CfCB | CfCB |CfCB.
action@description@stores data
=
store@store field@yes
store_true@store field if filter is true@yes
discard_false@discard message if filter is false@no
discard_true@discard message if filter is true@no
.TE
.PP
Do note that some filters require an additional data field:
.TS
box, center, tab (@);
 c | c | c
CfCB | CfCB | CfCB.
filter@description@data
=
noop@return true@-
exists@does the json_pointer point to an existing field@-
match@compare result of json_pointer against a string@string
substr@find string in result@string
.TE
.PP
This functionality is useful if you want only a subset of the data,
for example only non-error messages of customers with Doe in their name:
.PP
producers = (
    {
        type = "exports"
        threads = 1;
        topic = "data";
        jpointers = (
            [ "/error", "text", "discard_true", "exists" ],
            [ "/data/customer", "text", "store_true", "substr", "doe" ],
            "/data/request/http_response",
            "/data/request/body/0",
            ["/timestamp", "timestamp"],
        );
    } );
.RE
.PP
Because arrays are not human readable, a filter can also be declared in
a group. Default values can be omitted. This example is functionally
equivalent to the last:
producers = (
    {
        type = "exports"
        threads = 1;
        topic = "data";
        jpointers = (
            {
                jpointer = "/error";
                action = "discard_true";
                filter = "exists";
            },
            {
                jpointer = "/data/customer";
                action = "store_true";
                filter = "substr";
                data = "doe";
            },
            "/data/request/http_response",
            "/data/request/body/0",
            ["/timestamp", "timestamp"],
        );
    } );
.RE

.SH DATA PROCESSING
.SS messages
Messages are schaufels abstraction data envelopes used for queueing. They
hold the \fBmessage data\fR, an \fBxmark\fR for routing purposes (mark
a message for a specific consumer) and \fBmetadata\fR.
.PP
Metadata can be used by producers, consumers and hooks to implement various
features like callback functions, dynamic routing or even the transfer of
the original metadata of a message (think transfering a kafka timestamp).
.PP
Due to the implementation, a message can only hold 8 metadata points.

.SS xmark routing
Schaufel can route messages depending on a property called \fBxmark\fR.
Xmarks can be set on a message using the \fBxmark\fR hook. A producer will
then match this xmark and take it from the queue.
.PP
The standard \fBxmark\fR is \fI0\fR.
.PP
If you are using this feature, you are responsible for providing a producer
for each xmark you assign. If you do not do this, you will fill up the queue!

.SS hooks
Hooks are a way of transforming messages (mangling) on their way through
schaufel. At the moment, there are three hook kinds:
.TS
box, center, tab (@);
 c | c
CfCB | CfCB.
hook@description
=
dummy@return true
xmark@mark message with xmark
jsonexport@turn json into postgres binary
.TE
.PP
Hooks can be added to four possible places:

.TS
box, center, tab (@);
 c | c | c
CfCB | CfCB | CfCB.
position@hook name@execution point
=
consumer@hooks@before adding to the queue
queue@postadd@when the message arrives in the queue
queue@preget@when the message is taken from the queue
producer@hooks@after message is taken from queue
.TE

Important is that consumer/producer hooks run in a local context,
whereas queue hooks run in a global context (all messages are touched by them).
.PP
This (nonsensical) example writes all data to stdout and none to the file:
.RS
queue =
{
    preadd = (
        {
            type = "xmark";
            xmark = 1;
        }
    );
};

producers =
(
    {
        xmark = 1;
        type = "dummy";
    },
    {
        type = "file";
        file = "/tmp/out";
    }
);
.RE
.PP

.SS xmark
The xmark hook can mark a message with an xmark. It can either mark statically
with an xmark, or dynamically.
.PP
Dynamic marking works by selecting a metadata field, and then applying a hash
function to it. At the moment only \fBfnv32a_str\fR and \fBfnv32a_int\fR are
defined hashes. This will result in a 32 bit hash. Since this is too large
for any application, this can be \fIxor folded to a power of 2\fR.
.PP
Fold functions that fold to powers of to are called \fBfold\fR\fIn\fR.
For example, \fBfold2\fR will output xmarks 0 and 1. \ffold16\fR will
output xmarks between 0 and 65535.
.PP
.RS
queue = {
    postadd = (
        {
            type = "xmark";
            field = "jpointer";
            hash = "fnv32a_str";
            fold = "fold2"
            xmark = 1;
        }
    );
};

.RE
.PP
In the future, murmur as a hash <https://en.wikipedia.org/wiki/MurmurHash>
and ketama <https://github.com/RJ/ketama> as a consistent hashing system
could be added.

.SS jsonexport
\fBjsonexport\fR as a hook is equivalent to \fBexports\fR (as exports
duplicated code with the postgres producer and there is no conceivable
consumer).
.PP
It only adds one feature over exports, it can add a \fBmetadata\fR field to the
message if you use \fBstore_meta\fR as an action. The metadata field will be
called \fBjpointer\fR.
.RS
.PP
producers = ( {
    type = redis;
    host = "localhost:6379";
    topic = "events";
    hooks = (
        {
            type = "jsonexport";
            jpointers = (
                [ "/token", "text", "store_meta" ],
                "/message",
                [ "/timestamp", "timestamp" ]
            );
        }
    );
}
);
.RE

.SS PLEASE CONTRIBUTE
.PP
As always, feel free to implement more of what you need.
